{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bug_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sayeed910/nlp/blob/master/bug_classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XDtj9ZF2crih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a1dcc94a-c211-4c4b-ef10-6192b8c35280"
      },
      "cell_type": "code",
      "source": [
        "!pip install xlrd"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.6/dist-packages (1.1.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-b4cdg5AxUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pprint import pprint\n",
        "df = pd.read_excel('https://raw.githubusercontent.com/sayeed910/nlp/master/data/Mozilla_4source.xlsx', header=None, names=['severity', 'summary', 'description'], usecols=[0, 1, 2])\n",
        "\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Rf9L0fgh471",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "925c21db-9e1c-4fc2-85e6-18684b2ee5ac"
      },
      "cell_type": "code",
      "source": [
        "severities = df['severity']\n",
        "class_counts = severities.value_counts(ascending=True)\n",
        "\n",
        "print(class_counts)\n",
        "\n",
        "total_count = len(severities)\n",
        "class_probability = [0] * 6\n",
        "for i in range(1, 6):\n",
        "    class_probability[i] = class_counts.loc[i] / total_count\n",
        "    \n",
        "class_probability"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    283\n",
            "5    446\n",
            "2    508\n",
            "4    697\n",
            "3    701\n",
            "Name: severity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0.10740037950664137,\n",
              " 0.19278937381404174,\n",
              " 0.266034155597723,\n",
              " 0.2645161290322581,\n",
              " 0.16925996204933585]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "metadata": {
        "id": "JltyLfSfaqKd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nzeroes(n):\n",
        "    return [0] * n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZTagUjLaqKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "class BugClassifier:\n",
        "    \n",
        "    def __init__(self, class_count=5):\n",
        "        self.class_count = class_count\n",
        "        self.class_probability = nzeroes(class_count+1)\n",
        "        self.word_count_per_class = defaultdict(lambda : nzeroes(class_count+1))\n",
        "        self.total_words_per_class = nzeroes(class_count+1)\n",
        "        self.vocabulary = set()\n",
        "        self.p_word_given_class = defaultdict(lambda : nzeroes(class_count+1))\n",
        "        \n",
        "    def _model(self):\n",
        "        return (self.class_probability, self.word_count_per_class, self.vocabulary, self.total_words_per_class)\n",
        "    \n",
        "    def _calculate_class_probability(self, severities):\n",
        "        class_counts = severities.value_counts(ascending=True)        \n",
        "        total_count = len(severities)\n",
        "\n",
        "        for i in range(1, (self.class_count+1)):\n",
        "            self.class_probability[i] = class_counts.loc[i] / total_count\n",
        "        \n",
        "    \n",
        "    def _p_words_given_class(self, words, clazz):\n",
        "        \"P(Word|Class) = (Occurence of word in class + 1) / (total words in class + total words in vocabulary)\"\n",
        "\n",
        "        denominator = self.total_words_per_class[clazz] + len(self.vocabulary)\n",
        "        \n",
        "        return (((self.word_count_per_class[word][clazz] + 1) / denominator) * 100 for word in words)\n",
        "    \n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "    def fit(self, sentences, labels):\n",
        "            \n",
        "        self._calculate_class_probability(labels)\n",
        "        \n",
        "        for severity, sentence in zip(labels, sentences):            \n",
        "            words = sentence.split()\n",
        "            self.total_words_per_class[severity] += len(words)\n",
        "            \n",
        "            for word in words:\n",
        "                self.vocabulary.add(word)\n",
        "                self.word_count_per_class[word][severity] += 1\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "    def predict(self, sentences):\n",
        "        \n",
        "        return [self._predict(sentence) for sentence in sentences]\n",
        "        \n",
        "    \n",
        "    \n",
        "    def _predict(self, sentence):\n",
        "        \"\"\"\n",
        "        P(Class|Words) = P(Word_1|Class) * P(Word_2|Class) ... * P(Class)\n",
        "\n",
        "        class = argmax(P(Class|Words))\n",
        "        \"\"\"\n",
        "\n",
        "        words = sentence.split()\n",
        "        \n",
        "        p_class_given_sentence = [0] + \\\n",
        "            [reduce(operator.mul,\n",
        "                 self._p_words_given_class(words, clazz), self.class_probability[clazz])\n",
        "                        for clazz in range(1, (self.class_count + 1))]\n",
        "        \n",
        "        return p_class_given_sentence.index(max(p_class_given_sentence))\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MvHmLa5K2DiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def measure_accuracy(predicted_labels, real_labels, class_count=5):\n",
        "    confusion_matrix = [[0, 0, 0, 0, 0, 0] for _ in range(class_count+1)]\n",
        "        \n",
        "    for prediction, real in zip(predicted_labels, real_labels):\n",
        "        confusion_matrix[prediction][real] += 1\n",
        "        \n",
        "    precisions = nzeroes(class_count+1)\n",
        "    recalls = nzeroes(class_count+1)\n",
        "    for clazz in range(1, class_count+1):        \n",
        "        precisions[clazz] = confusion_matrix[clazz][clazz] / sum(confusion_matrix[clazz])\n",
        "        recalls[clazz] = confusion_matrix[clazz][clazz] / sum([row[clazz] for row in confusion_matrix])\n",
        "    \n",
        "#     print(precisions)\n",
        "#     print(recalls)\n",
        "    \n",
        "    precision = sum(precisions) / (len(precisions) - 1)  #the array is 1 size larger\n",
        "    recall = sum(recalls) / (len(recalls) - 1)  #the array is 1 size larger\n",
        "    f_measure = lambda measure: (2 * measure[0] * measure[1]) / (measure[0] + measure[1] + .0000001)\n",
        "    \n",
        "    return list(map(f_measure, zip(precisions[1:], recalls[1:])))\n",
        "   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQzW1hMsaqKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "1e5e75c6-63bd-4705-a086-611da146b086"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "total_rows = len(df)\n",
        "nrows_per_fold = total_rows // 11\n",
        "\n",
        "f_measures_summary = []\n",
        "f_measures_description = []\n",
        "\n",
        "for i in range(1, 11):\n",
        "    classifier = BugClassifier()\n",
        "    train_data_index = i * nrows_per_fold\n",
        "    train_data = df[: train_data_index]\n",
        "    test_data = df[train_data_index: ]\n",
        "    \n",
        "    classifier.fit(train_data['summary'], train_data['severity'])\n",
        "    predictions = classifier.predict(test_data['summary'])\n",
        "    \n",
        "    f_measures_summary.append(measure_accuracy(predictions, test_data['severity']))\n",
        "    \n",
        "    classifier = BugClassifier()\n",
        "    classifier.fit(train_data['description'], train_data['severity'])\n",
        "    predictions = classifier.predict(test_data['description'])\n",
        "    \n",
        "    f_measures_description.append(measure_accuracy(predictions, test_data['severity']))\n",
        "    \n",
        "\n",
        "avg_fmeasure_summary = [sum((row[i] for row in f_measures_summary)) / 10 for i in range(5)]\n",
        "avg_fmeasure_description = [sum((row[i] for row in f_measures_description)) / 10 for i in range(5)]\n",
        "\n",
        "print(\"F Measure(Summary):\", avg_fmeasure_summary)\n",
        "print(\"F Measure(Description):\", avg_fmeasure_description)\n"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F Measure(Summary): [0.5977984162507026, 0.5663075606530399, 0.4913573358906298, 0.47227946022575795, 0.3808233998525301]\n",
            "F Measure(Description): [0.621653832385726, 0.5026126723183791, 0.3982295297094724, 0.5126821318848307, 0.31844189579711096]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
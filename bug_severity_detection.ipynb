{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4257, [0, 2567, 6710, 14360, 7937, 4515])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit(df):\n",
    "    severity = df['severity']\n",
    "    counts = severity.value_counts(ascending=True)\n",
    "\n",
    "    total_count = len(severity)\n",
    "\n",
    "    class_probability = [0] * 6\n",
    "\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        class_probability[i] = counts.loc[i] / total_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class_group = df.groupby('severity')\n",
    "\n",
    "    from collections import defaultdict\n",
    "    word_count_per_class = defaultdict(lambda : [0, 0, 0, 0, 0, 0])\n",
    "    total_words_per_class = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    vocabulary = set()\n",
    "    for key, group in class_group:    \n",
    "        summaries = group['summary']    \n",
    "        for summary in summaries:        \n",
    "            words = summary.split()        \n",
    "            total_words_per_class[key] += len(words)\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "                word_count_per_class[word][key] += 1\n",
    "\n",
    "    \n",
    "    return class_probability, word_count_per_class, vocabulary, total_words_per_class\n",
    "        \n",
    "\n",
    "(len(vocabulary), total_words_per_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def measure_accuracy(test_data, class_probability, word_count_per_class, vocabulary, total_words_per_class):\n",
    "    \n",
    "    def probability_of_word_given_class(words, clazz):\n",
    "        \"P(Word|Class) = (Occurence of word in class + 1) / (total words in class + total words in vocabulary)\"\n",
    "\n",
    "        denominator = total_words_per_class[clazz] + len(vocabulary)\n",
    "\n",
    "        for word in words:\n",
    "            yield (word_count_per_class[word][clazz] + 1) / denominator\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict_severity(summary):\n",
    "        \"\"\"\n",
    "        P(Class|Words) = P(Word_1|Class) * P(Word_2|Class) ... * P(Class)\n",
    "\n",
    "        class = argmax(P(Class|Words))\n",
    "        \"\"\"\n",
    "\n",
    "        words = summary.split()\n",
    "        class_probability_given_summary = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        for clazz in range(1,6):\n",
    "            product_of_words_probability = (reduce(operator.mul, probability_of_word_given_class(words, clazz)))\n",
    "\n",
    "            class_probability_given_summary[clazz] = product_of_words_probability  * class_probability[clazz]\n",
    "\n",
    "        return class_probability_given_summary.index(max(class_probability_given_summary))\n",
    "\n",
    "    \n",
    "    \n",
    "    result_matrix = [[0, 0, 0, 0, 0, 0] for _ in range(6)]\n",
    "        \n",
    "    for index, data in test_data.iterrows():\n",
    "        prediction = predict_severity(data['summary'])\n",
    "        real = data['severity']\n",
    "        result_matrix[prediction][real] += 1\n",
    "        \n",
    "#     pprint([row[1:] for row in result_matrix[1:]])\n",
    "#     pprint(result_matrix)\n",
    "    \n",
    "    \n",
    "    precisions = [0] * 6\n",
    "    recalls = [0] * 6\n",
    "    for clazz in range(1, 6):        \n",
    "        precisions[clazz] = result_matrix[clazz][clazz] / sum(result_matrix[clazz])\n",
    "        recalls[clazz] = result_matrix[clazz][clazz] / sum([row[clazz] for row in result_matrix])\n",
    "    \n",
    "    \n",
    "    \n",
    "    precision = sum(precisions) / (len(precisions) - 1)  #the array is 1 size larger\n",
    "    recall = sum(recalls) / (len(recalls) - 1)  #the array is 1 size larger\n",
    "    f_measure = lambda measure: (2 * measure[0] * measure[1]) / (measure[0] + measure[1])\n",
    "    \n",
    "    return list(map(f_measure, zip(precisions[1:], recalls[1:])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09215686274509804,\n",
      "  0.2739244951712028,\n",
      "  0.37616822429906543,\n",
      "  0.24980959634424982,\n",
      "  0.13256006628003314],\n",
      " [0.148479427549195,\n",
      "  0.2657641610260064,\n",
      "  0.39447125932426497,\n",
      "  0.2976827094474153,\n",
      "  0.19716206123973112],\n",
      " [0.1442495126705653,\n",
      "  0.2570937231298366,\n",
      "  0.44856815578465065,\n",
      "  0.3044943820224719,\n",
      "  0.2636289666395444],\n",
      " [0.14984391259105098,\n",
      "  0.24281150159744408,\n",
      "  0.46979530703944083,\n",
      "  0.2921348314606742,\n",
      "  0.2801556420233463],\n",
      " [0.14639905548996457,\n",
      "  0.25138632162661734,\n",
      "  0.4689298043728423,\n",
      "  0.30965682362330404,\n",
      "  0.3046044864226682],\n",
      " [0.14392059553349876,\n",
      "  0.22549019607843138,\n",
      "  0.4644194756554307,\n",
      "  0.32622798887859134,\n",
      "  0.28484848484848485],\n",
      " [0.11487481590574375,\n",
      "  0.229038854805726,\n",
      "  0.4555789473684211,\n",
      "  0.33651551312649164,\n",
      "  0.28225806451612906],\n",
      " [0.13430127041742287,\n",
      "  0.17923186344238978,\n",
      "  0.46271094175285793,\n",
      "  0.3367003367003367,\n",
      "  0.25806451612903225],\n",
      " [0.1818181818181818,\n",
      "  0.17040358744394618,\n",
      "  0.47200000000000003,\n",
      "  0.33139534883720934,\n",
      "  0.2282608695652174],\n",
      " [0.2198581560283688,\n",
      "  0.17061611374407584,\n",
      "  0.48916408668730654,\n",
      "  0.29370629370629375,\n",
      "  0.18750000000000003]]\n",
      "[0.14759017907490898, 0.22657608180656763, 0.45018062022842803, 0.30783238241470384, 0.24190431576641863]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "df = pd.read_excel('data/Eclipse_4sourcev1.xls', header=None, names=['severity', 'summary', 'description'], usecols=[0, 1, 2])\n",
    "\n",
    "total_rows = len(df)\n",
    "nrows_per_fold = total_rows // 11\n",
    "\n",
    "f_measures = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    train_data_index = i * nrows_per_fold\n",
    "    train_data = df[: train_data_index]\n",
    "    test_data = df[train_data_index: ]\n",
    "    \n",
    "    class_probability, word_count_per_class, vocabulary, total_words_per_class = fit(train_data)\n",
    "    f_measures.append(measure_accuracy(test_data, class_probability, word_count_per_class, vocabulary, total_words_per_class))\n",
    "\n",
    "avg_fmeasure = []    \n",
    "for i in range(5):\n",
    "    avg_fmeasure.append(sum((row[i] for row in f_measures)) / 10)\n",
    "\n",
    "pprint(f_measures)\n",
    "print(avg_fmeasure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7373"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
